# -*- coding: utf-8 -*-
"""ANN_Model_Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Eg4N6ZW6617TL-nKOik-7olagUSrd8FF
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
data = pd.read_csv(url, sep=';')

X = data.drop('quality', axis=1)
y = data['quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=1)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error on Test Set: {mse:.4f}')

import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='sigmoid'))
model.add(Dense(32, activation='sigmoid'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=1)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error on Test Set with Sigmoid Activation: {mse:.4f}')

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Sigmoid Activation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='tanh'))
model.add(Dense(32, activation='tanh'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=1)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error on Test Set with Tanh Activation: {mse:.4f}')

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Tanh Activation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU
import matplotlib.pyplot as plt

model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1]))
model.add(LeakyReLU(alpha=0.01))
model.add(Dense(32))
model.add(LeakyReLU(alpha=0.01))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=1)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error on Test Set with Leaky ReLU Activation: {mse:.4f}')

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Leaky ReLU Activation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='softmax'))
model.add(Dense(32, activation='softmax'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=1)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error on Test Set with Softmax Activation: {mse:.4f}')

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Softmax Activation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='swish'))
model.add(Dense(32, activation='swish'))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='adam', loss='mean_squared_error')

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=1)

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error on Test Set with Swish Activation: {mse:.4f}')

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss (Swish Activation)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""# **COMPARISON OF THE ACTIVATION FUNCTIONS**"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU
import matplotlib.pyplot as plt


url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
data = pd.read_csv(url, sep=';')


X = data.drop('quality', axis=1)
y = data['quality']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


def evaluate_model(activation_fn, activation_name):
    print(f"\nTraining model with {activation_name} activation...")


    model = Sequential()
    if activation_name == 'Leaky ReLU':
        model.add(Dense(64, input_dim=X_train.shape[1]))
        model.add(LeakyReLU(alpha=0.01))
        model.add(Dense(32))
        model.add(LeakyReLU(alpha=0.01))
    else:
        model.add(Dense(64, input_dim=X_train.shape[1], activation=activation_fn))
        model.add(Dense(32, activation=activation_fn))

    model.add(Dense(1, activation='linear'))

    model.compile(optimizer='adam', loss='mean_squared_error')

    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=0)

    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f'Mean Squared Error on Test Set with {activation_name} Activation: {mse:.4f}')

    return mse, history.history['loss'], history.history['val_loss']

activations = [
    ('swish', 'Swish'),
    ('softmax', 'Softmax'),
    ('tanh', 'Tanh'),
    ('sigmoid', 'Sigmoid'),
    ('Leaky ReLU', 'Leaky ReLU')
]

results = {}


for activation_fn, activation_name in activations:
    if activation_name == 'Leaky ReLU':
        mse, train_loss, val_loss = evaluate_model(activation_fn, activation_name)
    else:
        mse, train_loss, val_loss = evaluate_model(activation_fn, activation_name)

    results[activation_name] = {
        'mse': mse,
        'train_loss': train_loss,
        'val_loss': val_loss
    }


plt.figure(figsize=(10, 5))
mse_values = [results[act]['mse'] for act in results]
plt.bar(results.keys(), mse_values, color=['blue', 'orange', 'green', 'red', 'purple'])
plt.title('Comparative Mean Squared Error of Different Activation Functions')
plt.xlabel('Activation Function')
plt.ylabel('Mean Squared Error')
plt.show()


plt.figure(figsize=(15, 10))
for i, (activation_name, result) in enumerate(results.items(), 1):
    plt.subplot(3, 2, i)
    plt.plot(result['train_loss'], label='Training Loss')
    plt.plot(result['val_loss'], label='Validation Loss')
    plt.title(f'{activation_name} Activation')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense


url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
data = pd.read_csv(url, sep=';')

X = data.drop('quality', axis=1)
y = data['quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


model = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=1)


y_pred = model.predict(X_test)


mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error on Test Set: {mse:.4f}')

plt.figure(figsize=(12, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


plt.figure(figsize=(12, 5))
plt.plot(y_test.values, label='Actual Quality', marker='o', linestyle='-', color='blue', alpha=0.7)
plt.plot(y_pred, label='Predicted Quality', marker='x', linestyle='--', color='red', alpha=0.7)
plt.title('Comparison of Predicted and Actual Wine Quality')
plt.xlabel('Test Sample Index')
plt.ylabel('Wine Quality')
plt.legend()
plt.show()

plt.figure(figsize=(12, 5))

indices = np.arange(0, len(y_test), step=5)
y_test_subset = y_test.values[indices]
y_pred_subset = y_pred[indices]

plt.plot(indices, y_test_subset, label='Actual Quality', marker='o', linestyle='-', color='blue', alpha=0.7)
plt.plot(indices, y_pred_subset, label='Predicted Quality', marker='x', linestyle='--', color='red', alpha=0.7)
plt.title('Comparison of Predicted and Actual Wine Quality')
plt.xlabel('Test Sample Index (spaced)')
plt.ylabel('Wine Quality')
plt.legend()
plt.show()

from sklearn.metrics import r2_score

r2 = r2_score(y_test, y_pred)
print(f'RÂ² Score on Test Set: {r2:.4f}')

!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo


wine_quality = fetch_ucirepo(id=186)


X = wine_quality.data.features
y = wine_quality.data.targets


print(wine_quality.metadata)


print(wine_quality.variables)

#final model

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report
from ucimlrepo import fetch_ucirepo
import matplotlib.pyplot as plt

wine_quality = fetch_ucirepo(id=186)
X = wine_quality.data.features
y = wine_quality.data.targets.astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = RandomForestClassifier(
    n_estimators=200,
    max_depth=12,
    class_weight="balanced",
    random_state=42
)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print("\n--- Model Evaluation ---")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

metrics = {'Accuracy': accuracy, 'F1 Score': f1, 'Recall': recall}
plt.figure(figsize=(8, 5))
plt.bar(metrics.keys(), metrics.values(), color=['skyblue', 'orange', 'lightgreen'])
plt.title('Model Evaluation Metrics')
plt.ylabel('Score')
plt.show()

#custom input

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report
from ucimlrepo import fetch_ucirepo
import matplotlib.pyplot as plt

wine_quality = fetch_ucirepo(id=186)
X = wine_quality.data.features
y = wine_quality.data.targets.astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = RandomForestClassifier(n_estimators=200, max_depth=12, class_weight="balanced", random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print("\n--- Model Evaluation ---")
print(f"Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

metrics = {'Accuracy': accuracy, 'F1 Score': f1, 'Recall': recall}
plt.figure(figsize=(8, 5))
plt.bar(metrics.keys(), metrics.values(), color=['skyblue', 'orange', 'lightgreen'])
plt.title('Model Evaluation Metrics')
plt.ylabel('Score')
plt.show()

def predict_custom_input(model, scaler):
    print("\n--- Custom Wine Quality Prediction ---")
    print("Enter values for the following features (comma-separated):")
    feature_names = list(X.columns)
    print(", ".join(feature_names))
    input_values = input("Enter values: ")

    try:
        input_data = np.array([float(value) for value in input_values.split(',')]).reshape(1, -1)
        input_data_scaled = scaler.transform(input_data)
        predicted_quality = model.predict(input_data_scaled)

        print("\n--- Prediction Result ---")
        print(f"Predicted Wine Quality: {predicted_quality[0]}")

        print("\n--- Model Insights ---")
        print(f"Model Accuracy on Test Set: {accuracy:.4f}")
        print(f"Model F1 Score on Test Set: {f1:.4f}")
        print(f"Model Recall on Test Set: {recall:.4f}")
        print("\n--- Detailed Information ---")
        print("Wine quality is rated on a scale of 0-10, where higher values indicate better quality. The features provided represent different chemical compositions and attributes of the wine such as acidity, alcohol content, and residual sugar. The model uses these features to predict the wine's quality.")

        print("\n--- Example of Feature Descriptions ---")
        print(f"Fixed Acidity: Measures the acidity of the wine in g/t. A higher value indicates more acidic wine.")
        print(f"Volatile Acidity: Measures the amount of acetic acid in the wine. Too much can indicate spoilage.")
        print(f"Citric Acid: Adds freshness and flavor. A higher level may indicate a fruity taste.")
        print(f"Residual Sugar: The amount of sugar left after fermentation. Wines with higher residual sugar tend to taste sweeter.")
        print(f"Chlorides: Measures the salt content of the wine. Higher levels may indicate spoilage.")
        print(f"Free Sulfur Dioxide: Preservative used to prevent oxidation. A higher level may indicate preservation, but excessive levels can be harmful.")
        print(f"Total Sulfur Dioxide: The total amount of sulfur dioxide present. Similar to free sulfur dioxide, but includes bound forms.")
        print(f"Density: Measures the mass of the wine per unit volume. Can reflect the alcohol and sugar content.")
        print(f"pH: Indicates the acidity or alkalinity of the wine. Wines with lower pH tend to be more acidic.")
        print(f"Sulphates: A component that affects the wine's flavor and taste. Higher values can contribute to a stronger taste.")
        print(f"Alcohol: The percentage of alcohol in the wine. Higher alcohol content can influence the flavor profile.")

    except Exception as e:
        print("Error: Please enter valid input values.")
        print(e)

predict_custom_input(model, scaler)

